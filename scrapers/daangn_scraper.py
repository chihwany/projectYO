"""
ë‹¹ê·¼ (Daangn/Karrot) ìŠ¤í¬ë˜í¼
www.daangn.com ë§¤ë¬¼ì„ ê²€ìƒ‰í•˜ê³  ì¡°íšŒí•˜ëŠ” ëª¨ë“ˆ
"""

import datetime
import requests
import json
import time
import re
from urllib.parse import quote, urlencode, unquote
from typing import Optional
from bs4 import BeautifulSoup


class DaangnScraper:
    """ë‹¹ê·¼ ë§¤ë¬¼ ìŠ¤í¬ë˜í¼"""

    BASE_URL = "https://www.daangn.com"

    CATEGORY_MAP = {
        1: "ë””ì§€í„¸ê¸°ê¸°",
        172: "ìƒí™œê°€ì „",
        8: "ê°€êµ¬/ì¸í…Œë¦¬ì–´",
        7: "ìƒí™œ/ì£¼ë°©",
        4: "ìœ ì•„ë™",
        173: "ìœ ì•„ë„ì„œ",
        5: "ì—¬ì„±ì˜ë¥˜",
        31: "ì—¬ì„±ì¡í™”",
        14: "ë‚¨ì„±íŒ¨ì…˜/ì¡í™”",
        6: "ë·°í‹°/ë¯¸ìš©",
        3: "ìŠ¤í¬ì¸ /ë ˆì €",
        2: "ì·¨ë¯¸/ê²Œì„/ìŒë°˜",
        9: "ë„ì„œ",
        304: "í‹°ì¼“/êµí™˜ê¶Œ",
        517: "eì¿ í°",
        305: "ê°€ê³µì‹í’ˆ",
        483: "ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ",
        16: "ë°˜ë ¤ë™ë¬¼ìš©í’ˆ",
        139: "ì‹ë¬¼",
        13: "ê¸°íƒ€ ì¤‘ê³ ë¬¼í’ˆ",
        32: "ì‚½ë‹ˆë‹¤",
    }

    def __init__(self, delay: float = 1.0):
        self.delay = delay
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "Accept-Encoding": "gzip, deflate, br",
            "Referer": "https://www.daangn.com/",
        })
        self._last_request_time = 0
        self._region_cache = None  # ì§€ì—­ ìºì‹œ (flat list)

    def _parse_time_ago(self, time_ago_str: str) -> Optional[datetime.datetime]:
        """
        'Xë¶„ ì „', 'Xì‹œê°„ ì „', 'Xì¼ ì „', 'Xì£¼ ì „', 'Xê°œì›” ì „' ë“±ì˜ ë¬¸ìì—´ì„
        ì ˆëŒ€ datetime ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        """
        now = datetime.datetime.now()

        if not time_ago_str:
            return None

        time_ago_str = time_ago_str.strip()
        # Handle "ëŒì˜¬" prefix
        if time_ago_str.startswith("ëŒì˜¬ "):
            time_ago_str = time_ago_str[3:] # Remove "ëŒì˜¬ " (3 characters + space)
        
        if "ë°©ê¸ˆ ì „" in time_ago_str:
            return now
        
        match = re.match(r"(\d+)(ë¶„|ì‹œê°„|ì¼|ì£¼|ê°œì›”) ì „", time_ago_str)
        if match:
            value = int(match.group(1))
            unit = match.group(2)

            if unit == "ë¶„":
                return now - datetime.timedelta(minutes=value)
            elif unit == "ì‹œê°„":
                return now - datetime.timedelta(hours=value)
            elif unit == "ì¼":
                return now - datetime.timedelta(days=value)
            elif unit == "ì£¼":
                return now - datetime.timedelta(weeks=value)
            elif unit == "ê°œì›”":
                # Assuming 1 month = 30 days for simplicity
                return now - datetime.timedelta(days=value * 30)
        
        # If it's not a relative time string, return None
        return None

    def _throttle(self):
        elapsed = time.time() - self._last_request_time
        if elapsed < self.delay:
            time.sleep(self.delay - elapsed)
        self._last_request_time = time.time()

    def _resolve_region(self, region: str) -> str:
        """
        ì§€ì—­ëª…ì„ ë‹¹ê·¼ ì½”ë“œë¡œ ë³€í™˜.
        ì´ë¯¸ 'ì§€ì—­ëª…-ID' í˜•ì‹ì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜.
        ì•„ë‹ˆë©´ ìºì‹œì—ì„œ ê²€ìƒ‰í•˜ì—¬ ë§¤ì¹­.
        """
        if not region:
            return ""

        # ì´ë¯¸ ì½”ë“œ í˜•ì‹ì´ë©´ ê·¸ëŒ€ë¡œ
        if "-" in region and region.rsplit("-", 1)[-1].isdigit():
            return region

        # ìºì‹œì—ì„œ ê²€ìƒ‰
        if self._region_cache is None:
            self._load_region_cache()

        if self._region_cache:
            # ì •í™•í•œ ì´ë¦„ ë§¤ì¹­ ìš°ì„ 
            for item in self._region_cache:
                if item["name"] == region:
                    return item["code"]
            # ë¶€ë¶„ ë§¤ì¹­
            for item in self._region_cache:
                if region in item["name"] or region in item["full"]:
                    return item["code"]

        # ìºì‹œì— ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì „ë‹¬ (ë‹¹ê·¼ì´ ì²˜ë¦¬)
        return region

    def _load_region_cache(self):
        """ì§€ì—­ ìºì‹œ íŒŒì¼ ë¡œë“œ"""
        try:
            from daangn_regions import load_regions, build_flat_list
            data = load_regions()
            if data:
                self._region_cache = build_flat_list(data)
            else:
                self._region_cache = []
        except ImportError:
            self._region_cache = []

    def _build_search_url(
        self,
        keyword: str,
        region: Optional[str] = None,
        category: Optional[int] = None,
        min_price: Optional[int] = None,
        max_price: Optional[int] = None,
        only_on_sale: bool = False,
        page: int = 1,
    ) -> str:
        params = {}

        if keyword:
            params["search"] = keyword

        if region:
            resolved = self._resolve_region(region)
            if resolved:
                params["in"] = resolved

        if category:
            params["category_id"] = category

        if min_price is not None and max_price is not None:
            params["price"] = f"{min_price}__{max_price}"
        elif min_price is not None:
            params["price"] = f"{min_price}__"
        elif max_price is not None:
            params["price"] = f"0__{max_price}"

        if only_on_sale:
            params["only_on_sale"] = "true"

        if page > 1:
            params["page"] = page

        url = f"{self.BASE_URL}/kr/buy-sell/s/"
        if params:
            url += "?" + urlencode(params)

        return url

    @staticmethod
    def _is_garbled(text: str) -> bool:
        """í…ìŠ¤íŠ¸ê°€ ê¹¨ì¡ŒëŠ”ì§€ í™•ì¸ (latin1ë¡œ ì˜ëª» ë””ì½”ë”©ëœ í•œê¸€)"""
        if not text:
            return False
        garbled_chars = sum(1 for c in text if 127 < ord(c) < 256)
        korean_chars = sum(1 for c in text if '\uac00' <= c <= '\ud7a3')
        if garbled_chars > 2 and korean_chars == 0:
            return True
        return False

    @staticmethod
    def _title_from_slug(href: str) -> str:
        """URL slugì—ì„œ ì œëª©ì„ ë³µì›"""
        slug = href.rstrip("/").split("/")[-1] if href else ""
        if not slug:
            return ""
        decoded = unquote(slug)
        parts = decoded.rsplit("-", 1)
        if len(parts) == 2 and len(parts[1]) >= 8 and re.match(r'^[a-z0-9]+$', parts[1]):
            decoded = parts[0]
        title = decoded.replace("-", " ").strip()
        return title

    def _parse_items_from_html(self, html: str) -> list[dict]:
        """HTML ë‚´ì˜ window.__remixContext JSONì—ì„œ ìƒí’ˆ ëª©ë¡ ì¶”ì¶œ"""
        results = []
        
        # 1. JSON ë°ì´í„° ì¶”ì¶œ ì‹œë„
        match = re.search(r"window\.__remixContext\s*=\s*({.*?});", html, re.DOTALL)
        if match:
            try:
                json_str = match.group(1)
                data = json.loads(json_str)
                
                # ê²½ë¡œ: state -> loaderData -> routes/kr.buy-sell.s -> allPage -> fleamarketArticles
                # (ì°¸ê³ : ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ì— ë”°ë¼ ê²½ë¡œê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
                loader_data = data.get("state", {}).get("loaderData", {})
                articles = []
                
                # ê°€ëŠ¥í•œ ê²½ë¡œë“¤ì„ íƒìƒ‰
                for route_key in loader_data:
                    if "buy-sell" in route_key:
                        articles = loader_data[route_key].get("allPage", {}).get("fleamarketArticles", [])
                        if articles:
                            break
                
                if articles:
                    for art in articles:
                        price_val = art.get("price", "0")
                        try:
                            price = int(float(price_val))
                        except (ValueError, TypeError):
                            price = 0
                            
                        # ì‹œê°„ ì •ë³´ (boostedAtì´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ createdAt)
                        time_str = art.get("boostedAt") or art.get("createdAt")
                        
                        results.append({
                            "id": art.get("id", "").strip("/").split("/")[-1],
                            "title": art.get("title"),
                            "price": price,
                            "price_str": "ë‚˜ëˆ”ğŸ§¡" if price == 0 else f"{price:,}ì›",
                            "image_url": art.get("thumbnail"),
                            "status": "íŒë§¤ì¤‘" if art.get("status") == "Ongoing" else art.get("status"),
                            "location": art.get("region", {}).get("name", ""),
                            "url": f"{self.BASE_URL}{art.get('id')}" if art.get("id") else "",
                            "time": time_str,
                            "time_ago": "ìµœê·¼" # JSONì—ëŠ” ìƒëŒ€ ì‹œê°„ì´ ì—†ìœ¼ë¯€ë¡œ ê¸°ë³¸ê°’ ì„¤ì •
                        })
                    return results
            except Exception as e:
                print(f"DEBUG: JSON parsing failed: {e}. Falling back to HTML parsing.")

        # 2. JSON ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ HTML íŒŒì‹± ë°©ì‹ (Fallback)
        soup = BeautifulSoup(html, "html.parser")
        product_links = soup.find_all("a", href=re.compile(r"/kr/buy-sell/(?!s[/?]|s$)[^?]"))

        for link in product_links:
            href = link.get("href", "")
            if href == "/kr/buy-sell/":
                continue

            img_tag = link.find("img")
            image_url = ""
            img_alt = ""
            if img_tag:
                image_url = img_tag.get("src", img_tag.get("data-src", ""))
                img_alt = img_tag.get("alt", "")

            text_content = link.get_text(separator="\n", strip=True)
            lines = [l.strip() for l in text_content.split("\n") if l.strip()]

            if not lines:
                continue

            status = "íŒë§¤ì¤‘"
            title_start = 0
            if lines[0] in ("ì˜ˆì•½ì¤‘", "íŒë§¤ì™„ë£Œ", "ê±°ë˜ì™„ë£Œ"):
                status = lines[0]
                title_start = 1

            title = lines[title_start] if len(lines) > title_start else ""
            if not title or title == "thumbnail":
                if img_alt and img_alt != "thumbnail":
                    title = img_alt
                else:
                    continue

            if self._is_garbled(title):
                slug_title = self._title_from_slug(href)
                if slug_title:
                    title = slug_title

            price = 0
            price_str = "ê°€ê²©ë¯¸ì •"
            for line in lines[title_start + 1:]:
                price_match = re.search(r'([\d,]+)\s*ì›', line)
                if price_match:
                    price = int(price_match.group(1).replace(",", ""))
                    price_str = f"{price:,}ì›"
                    break
                if "ë‚˜ëˆ”" in line:
                    price_str = "ë‚˜ëˆ”ğŸ§¡"
                    break

            if price == 0:
                price_in_title = re.search(r'([\d,]+)ì›$', title)
                if price_in_title:
                    price = int(price_in_title.group(1).replace(",", ""))
                    price_str = f"{price:,}ì›"
                    title = title[:price_in_title.start()].strip()

            location = ""
            time_ago_str = ""

            card_desc = link.find("div", class_="card-desc")
            if card_desc:
                card_desc_full_text = card_desc.get_text(separator=' ', strip=True)
                print(f"DEBUG: card_desc_full_text: '{card_desc_full_text}'") # Debug print

                # Pattern: (Location part) (optional Â· ) (optional ëŒì˜¬ ) (Time ago part)
                # Group 1: Location (e.g., "ì§„ì˜ì")
                # Group 2: Optional "ëŒì˜¬ " prefix
                # Group 3: Time ago string (e.g., "15ë¶„ ì „", "ë°©ê¸ˆ ì „")
                combined_pattern = r"([ê°€-í£\w]+[ìë©´ë™êµ¬ì‹œë¦¬])(?:\s*Â·?\s*(ëŒì˜¬\s*)?((?:\d+(?:ë¶„|ì‹œê°„|ì¼|ì£¼|ê°œì›”))? ì „|ë°©ê¸ˆ ì „))?"
                
                match = re.search(combined_pattern, card_desc_full_text)
                print(f"DEBUG: Regex match: {match}") # Debug print
                if match:
                    location = match.group(1)
                    time_ago_str_candidate = match.group(3)
                    if time_ago_str_candidate:
                        time_ago_str = time_ago_str_candidate
                print(f"DEBUG: Extracted location: '{location}', time_ago_str: '{time_ago_str}'") # Debug print
            
            full_url = f"{self.BASE_URL}{href}" if href.startswith("/") else href
            slug = href.rstrip("/").split("/")[-1] if href else ""

            results.append({
                "id": slug,
                "title": title,
                "price": price,
                "price_str": price_str,
                "image_url": image_url,
                "status": status,
                "location": location,
                "url": full_url,
                "time_ago": time_ago_str,
                "time": self._parse_time_ago(time_ago_str).isoformat() if self._parse_time_ago(time_ago_str) else None,
            })

        return results

    def search(
        self,
        keyword: str,
        region: Optional[str] = None,
        page: int = 1,
        category: Optional[int] = None,
        min_price: Optional[int] = None,
        max_price: Optional[int] = None,
        only_on_sale: bool = False,
    ) -> list[dict]:
        """
        ë‹¹ê·¼ ë§¤ë¬¼ ê²€ìƒ‰

        Args:
            keyword: ê²€ìƒ‰ì–´
            region: ì§€ì—­ëª… ë˜ëŠ” ì½”ë“œ (ì˜ˆ: "ì„œì´ˆ4ë™", "ê°•ë‚¨êµ¬", "ì„œì´ˆ4ë™-366")
            page: í˜ì´ì§€ ë²ˆí˜¸ (1ë¶€í„° ì‹œì‘)
            category: ì¹´í…Œê³ ë¦¬ ì½”ë“œ
            min_price: ìµœì†Œ ê°€ê²©
            max_price: ìµœëŒ€ ê°€ê²©
            only_on_sale: ê±°ë˜ ê°€ëŠ¥ë§Œ ë³´ê¸°

        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        self._throttle()

        url = self._build_search_url(
            keyword=keyword,
            region=region,
            category=category,
            min_price=min_price,
            max_price=max_price,
            only_on_sale=only_on_sale,
            page=page,
        )

        try:
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            response.encoding = "utf-8"

            # DEBUG: Save response HTML to a file for inspection
            with open("daangn_response.html", "w", encoding="utf-8") as f:
                f.write(response.text)
            print("DEBUG: Saved daangn_response.html for inspection.")
        except requests.RequestException as e:
            print(f"[ì˜¤ë¥˜] ìš”ì²­ ì‹¤íŒ¨: {e}")
            return []

        results = self._parse_items_from_html(response.text)

        region_str = f" ({region})" if region else ""
        if results:
            print(f"[ê²€ìƒ‰ì™„ë£Œ] '{keyword}'{region_str} - {len(results)}ê°œ ì¡°íšŒ")
        else:
            print(f"[ê²€ìƒ‰ì™„ë£Œ] '{keyword}'{region_str} - ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")

        return results

    def search_all(
        self,
        keyword: str,
        max_pages: int = 5,
        **kwargs,
    ) -> list[dict]:
        """ì—¬ëŸ¬ í˜ì´ì§€ì— ê±¸ì³ ê²€ìƒ‰"""
        all_results = []
        for page in range(1, max_pages + 1):
            results = self.search(keyword, page=page, **kwargs)
            if not results:
                break
            all_results.extend(results)
            print(f"  ... {page}í˜ì´ì§€ ì™„ë£Œ (ëˆ„ì  {len(all_results)}ê°œ)")
        return all_results

    @staticmethod
    def format_results(results: list[dict], show_url: bool = True) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ í¬ë§·íŒ…"""
        if not results:
            return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

        lines = []
        lines.append(f"{'='*60}")
        lines.append(f" ë‹¹ê·¼ ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ")
        lines.append(f"{'='*60}")

        for i, item in enumerate(results, 1):
            lines.append(f"\n[{i}] {item['title']}")
            lines.append(f"    ğŸ’° ê°€ê²©: {item['price_str']}")
            if item.get("status") and item["status"] != "íŒë§¤ì¤‘":
                lines.append(f"    ğŸ“Œ ìƒíƒœ: {item['status']}")
            if item.get("location"):
                lines.append(f"    ğŸ“ ì§€ì—­: {item['location']}")
            if show_url:
                lines.append(f"    ğŸ”— ë§í¬: {item['url']}")
            lines.append(f"    {'â”€'*40}")

        return "\n".join(lines)
